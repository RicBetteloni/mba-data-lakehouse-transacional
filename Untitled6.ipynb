{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3HHKuzsfjnX3"
      },
      "outputs": [],
      "source": [
        "# CÉLULA 1 - INSTALAÇÃO DAS BIBLIOTECAS\n",
        "\n",
        "!pip install -q pyspark==3.5.1 delta-spark==3.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 2 - CONFIGURAÇÃO DO SPARK + DELTA\n",
        "# Nesta etapa criamos a SparkSession já habilitando o suporte ao Delta Lake,\n",
        "# para conseguirmos usar as operações ACID e o formato open data na camada final.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "\n",
        "# Builder padrão do Spark, mas com as extensões do Delta ativadas.\n",
        "# Essas configs fazem o Spark enxergar o Delta como catálogo padrão.\n",
        "builder = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"MBA_DataLakehouse_Delta\")  # nome da aplicação no cluster\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        ")\n",
        "\n",
        "# Aqui o Delta ajusta a sessão Spark com as dependências necessárias.\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")  # reduz o ruído de log para focar no pipeline\n",
        "\n",
        "print(\"Spark inicializado\")\n",
        "print(\"Versão do Spark:\", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkXSnegukhNM",
        "outputId": "07b2ec51-15a1-40de-f914-de9f933c1e43"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark inicializado\n",
            "Versão do Spark: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 3 - COLETA/AQUISIÇÃO (CAMADA RAW - PARQUET)\n",
        "# Aqui simulamos a fonte transacional (logs de pedidos) e persistimos a camada RAW\n",
        "# em formato Parquet, conforme sugestão da zona bruta do lake.\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Caminho da camada RAW no filesystem do ambiente\n",
        "path_raw = \"/tmp/vendas_raw\"\n",
        "# Limpeza da pasta a cada execução para evitar lixo de runs anteriores\n",
        "shutil.rmtree(path_raw, ignore_errors=True)\n",
        "\n",
        "# Geração de 5000 pedidos simulados em memória\n",
        "# A ideia é imitar um fluxo de transações de e‑commerce/banking.\n",
        "random.seed(42)\n",
        "base_date = datetime(2024, 10, 1)\n",
        "rows = []\n",
        "\n",
        "for i in range(1, 5001):\n",
        "    # Aproximadamente 5% dos customer_id ficam nulos para simular problema de qualidade\n",
        "    customer_id = f\"CUST{random.randint(1, 100)}\" if random.random() > 0.05 else None\n",
        "    # Aproximadamente 3% dos amounts nulos\n",
        "    amount = round(random.uniform(5, 1000), 2) if random.random() > 0.03 else None\n",
        "    # Aproximadamente 2% das datas nulas\n",
        "    order_dt = base_date + timedelta(seconds=random.randint(0, 86400 * 60))\n",
        "    order_date = order_dt.strftime(\"%Y-%m-%d %H:%M:%S\") if random.random() > 0.02 else None\n",
        "    # Status transacional típico\n",
        "    status = random.choice([\"PENDING\", \"SHIPPED\", \"DELIVERED\", \"CANCELLED\"])\n",
        "\n",
        "    rows.append((f\"ORD{i:05d}\", customer_id, amount, order_date, status))\n",
        "\n",
        "# Crio o DataFrame diretamente da lista de tuplas (sem depender de Pandas)\n",
        "df_raw = spark.createDataFrame(\n",
        "    rows,\n",
        "    [\"order_id\", \"customer_id\", \"amount\", \"order_date\", \"status\"]\n",
        ")\n",
        "\n",
        "print(\"Registros totais gerados:\", df_raw.count())\n",
        "\n",
        "# Persisto a camada RAW usando Parquet, que é um formato coluna aberto e otimizado\n",
        "df_raw.write.mode(\"overwrite\").parquet(path_raw)\n",
        "print(\"Camada RAW salva em:\", path_raw)\n",
        "\n",
        "# Leitura de validação da RAW para garantir que os dados foram gravados corretamente\n",
        "df_check = spark.read.parquet(path_raw)\n",
        "print(\"Registros lidos da camada RAW:\", df_check.count())\n",
        "df_check.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZdsH4jGku2b",
        "outputId": "a68d2ab6-4e49-4123-e74a-444fa9acdffe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registros totais gerados: 5000\n",
            "Camada RAW salva em: /tmp/vendas_raw\n",
            "Registros lidos da camada RAW: 5000\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "|order_id|customer_id|amount|order_date         |status   |\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "|ORD02049|CUST37     |668.69|2024-10-25 03:05:23|PENDING  |\n",
            "|ORD02050|CUST4      |857.88|2024-10-27 15:00:19|DELIVERED|\n",
            "|ORD02051|CUST94     |163.69|2024-10-06 04:45:07|SHIPPED  |\n",
            "|ORD02052|CUST95     |80.72 |2024-11-10 12:48:55|DELIVERED|\n",
            "|ORD02053|CUST16     |266.41|2024-10-20 20:54:22|PENDING  |\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 4 - PRÉ-PROCESSAMENTO (3 TRANSFORMAÇÕES) + CAMADA CURATED (DELTA)\n",
        "# Nesta etapa parte da camada RAW e aplicamos as três transformações de qualidade\n",
        "# combinadas em aula: limpeza de nulos, ajuste de tipos e desduplicação.\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp, row_number\n",
        "from pyspark.sql.window import Window\n",
        "import shutil\n",
        "\n",
        "path_raw = \"/tmp/vendas_raw\"\n",
        "path_curated_delta = \"/tmp/vendas_curated_delta\"\n",
        "# Limpo a pasta da camada tratada para não misturar versões antigas\n",
        "shutil.rmtree(path_curated_delta, ignore_errors=True)\n",
        "\n",
        "# 0) Ler camada RAW\n",
        "df = spark.read.parquet(path_raw)\n",
        "print(\"Registros lidos da RAW:\", df.count())\n",
        "\n",
        "# 1) Limpeza de nulos: remover registros com amount OU order_date nulos\n",
        "#    Aqui eu trato qualidade: se valor financeiro ou data da transação estão faltando,\n",
        "#    o registro é considerado inválido para análises.\n",
        "antes = df.count()\n",
        "df = df.dropna(subset=[\"amount\", \"order_date\"])\n",
        "depois = df.count()\n",
        "print(\"\\n[1] Limpeza de nulos\")\n",
        "print(\"   Antes :\", antes)\n",
        "print(\"   Depois:\", depois)\n",
        "print(\"   Removidos:\", antes - depois)\n",
        "\n",
        "# 2) Casting de tipos: amount DECIMAL(10,2), order_date TIMESTAMP\n",
        "#    Transformo os tipos para formatos mais adequados:\n",
        "#    - amount como DECIMAL para cálculos financeiros precisos\n",
        "#    - order_date como TIMESTAMP para facilitar filtros e agregações por tempo\n",
        "df = df.withColumn(\"amount\", col(\"amount\").cast(\"DECIMAL(10,2)\"))\n",
        "df = df.withColumn(\"order_date\", to_timestamp(\"order_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        "\n",
        "print(\"\\n[2] Casting de tipos\")\n",
        "df.printSchema()\n",
        "\n",
        "# 3) Desduplicação: manter registro mais recente por order_id\n",
        "#    Uso uma Window Function para ordenar por data dentro de cada order_id\n",
        "#    e fico apenas com a linha mais recente (row_number == 1).\n",
        "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"order_date\").desc())\n",
        "antes_dedup = df.count()\n",
        "df = df.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
        "       .filter(col(\"rn\") == 1) \\\n",
        "       .drop(\"rn\")\n",
        "depois_dedup = df.count()\n",
        "\n",
        "print(\"\\n[3] Desduplicação\")\n",
        "print(\"   Antes :\", antes_dedup)\n",
        "print(\"   Depois:\", depois_dedup)\n",
        "print(\"   Removidos (duplicados):\", antes_dedup - depois_dedup)\n",
        "\n",
        "print(\"\\nAmostra após as 3 transformações:\")\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "# Salvar em Delta Lake (camada CURATED)\n",
        "# A partir daqui a tabela já está em um Open Table Format com suporte a ACID.\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(path_curated_delta)\n",
        "print(\"\\nCamada CURATED salva em Delta:\", path_curated_delta)\n",
        "print(\"Total de registros na CURATED:\", df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6DLeMfgk5sj",
        "outputId": "8b80ced2-4e22-4632-efca-c673cb9fc0f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registros lidos da RAW: 5000\n",
            "\n",
            "[1] Limpeza de nulos\n",
            "   Antes : 5000\n",
            "   Depois: 4737\n",
            "   Removidos: 263\n",
            "\n",
            "[2] Casting de tipos\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- amount: decimal(10,2) (nullable = true)\n",
            " |-- order_date: timestamp (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n",
            "\n",
            "[3] Desduplicação\n",
            "   Antes : 4737\n",
            "   Depois: 4737\n",
            "   Removidos (duplicados): 0\n",
            "\n",
            "Amostra após as 3 transformações:\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "|order_id|customer_id|amount|order_date         |status   |\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "|ORD00001|CUST4      |248.67|2024-10-14 13:08:48|PENDING  |\n",
            "|ORD00003|CUST92     |547.22|2024-10-22 09:39:49|DELIVERED|\n",
            "|ORD00004|CUST1      |163.86|2024-11-11 00:45:46|SHIPPED  |\n",
            "|ORD00005|CUST98     |97.28 |2024-10-10 09:21:55|DELIVERED|\n",
            "|ORD00006|CUST6      |538.55|2024-11-06 18:02:56|DELIVERED|\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Camada CURATED salva em Delta: /tmp/vendas_curated_delta\n",
            "Total de registros na CURATED: 4737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 5 - ANÁLISES DE NEGÓCIO + COMPARATIVO PARQUET vs DELTA\n",
        "# Nesta etapa exploramos a camada CURATED em Delta Lake com alguns KPIs\n",
        "# e, em seguida, comparamos o tempo de execução da mesma query\n",
        "#  rodando sobre a RAW em Parquet e sobre a CURATED em Delta.\n",
        "\n",
        "from pyspark.sql.functions import round as _round\n",
        "import time\n",
        "\n",
        "path_raw = \"/tmp/vendas_raw\"\n",
        "path_curated_delta = \"/tmp/vendas_curated_delta\"\n",
        "\n",
        "# -----------------------------\n",
        "# 5.1 Ler CURATED (Delta) e calcular KPIs\n",
        "# -----------------------------\n",
        "df_curated = spark.read.format(\"delta\").load(path_curated_delta)\n",
        "df_curated.createOrReplaceTempView(\"vendas_curated\")\n",
        "\n",
        "print(\"Total de registros na CURATED (Delta):\", df_curated.count())\n",
        "\n",
        "# KPI 1: visão geral do financeiro na tabela tratada em Delta\n",
        "print(\"\\n[KPI 1] Resumo financeiro geral (Delta)\")\n",
        "kpi1_delta = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) AS total_pedidos,\n",
        "        ROUND(SUM(amount), 2) AS valor_total,\n",
        "        ROUND(AVG(amount), 2) AS ticket_medio,\n",
        "        ROUND(MIN(amount), 2) AS menor_compra,\n",
        "        ROUND(MAX(amount), 2) AS maior_compra\n",
        "    FROM vendas_curated\n",
        "\"\"\")\n",
        "kpi1_delta.show(truncate=False)\n",
        "\n",
        "# KPI 2: distribuição de valor por status de pedido\n",
        "print(\"\\n[KPI 2] Valor total por status (Delta)\")\n",
        "kpi2_delta = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        status,\n",
        "        COUNT(*) AS qtd_pedidos,\n",
        "        ROUND(SUM(amount), 2) AS valor_total,\n",
        "        ROUND(AVG(amount), 2) AS ticket_medio\n",
        "    FROM vendas_curated\n",
        "    GROUP BY status\n",
        "    ORDER BY qtd_pedidos DESC\n",
        "\"\"\")\n",
        "kpi2_delta.show(truncate=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 5.2 Comparativo simples: Parquet (RAW) x Delta (CURATED)\n",
        "#     Mesma query, medindo tempo de execução\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n================ COMPARATIVO PARQUET x DELTA ================\")\n",
        "\n",
        "# Ler RAW em Parquet (camada bruta)\n",
        "df_raw = spark.read.parquet(path_raw)\n",
        "df_raw.createOrReplaceTempView(\"vendas_raw\")\n",
        "\n",
        "# Query de resumo financeiro (equivalente ao KPI 1),\n",
        "# parametrizada para eu poder trocar só o nome da tabela.\n",
        "query_resumo = \"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) AS total_pedidos,\n",
        "        ROUND(SUM(amount), 2) AS valor_total,\n",
        "        ROUND(AVG(amount), 2) AS ticket_medio\n",
        "    FROM {tabela}\n",
        "\"\"\"\n",
        "\n",
        "# Tempo no PARQUET (RAW)\n",
        "inicio_parquet = time.time()\n",
        "res_parquet = spark.sql(query_resumo.format(tabela=\"vendas_raw\"))\n",
        "tempo_parquet = time.time() - inicio_parquet\n",
        "\n",
        "print(\"\\n[PARQUET - RAW] Resumo financeiro\")\n",
        "res_parquet.show(truncate=False)\n",
        "print(f\"Tempo PARQUET: {tempo_parquet:.4f} segundos\")\n",
        "\n",
        "# Tempo no DELTA (CURATED)\n",
        "inicio_delta = time.time()\n",
        "res_delta = spark.sql(query_resumo.format(tabela=\"vendas_curated\"))\n",
        "tempo_delta = time.time() - inicio_delta\n",
        "\n",
        "print(\"\\n[DELTA - CURATED] Resumo financeiro\")\n",
        "res_delta.show(truncate=False)\n",
        "print(f\"Tempo DELTA:   {tempo_delta:.4f} segundos\")\n",
        "\n",
        "print(\"\\nObservação: aqui não buscamos benchmark rigoroso,\")\n",
        "print(\"mas evidenciar a diferença entre consultar a camada RAW em Parquet\")\n",
        "print(\"e a camada tratada em Delta Lake, que além do desempenho oferece ACID\")\n",
        "print(\"e melhor gerenciamento de metadados para o Lakehouse.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTDoxMrLlEbL",
        "outputId": "7c92bfa7-1f4b-4c45-a4d3-d2917afa9649"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de registros na CURATED (Delta): 4737\n",
            "\n",
            "[KPI 1] Resumo financeiro geral (Delta)\n",
            "+-------------+-----------+------------+------------+------------+\n",
            "|total_pedidos|valor_total|ticket_medio|menor_compra|maior_compra|\n",
            "+-------------+-----------+------------+------------+------------+\n",
            "|4737         |2406360.10 |507.99      |5.25        |999.99      |\n",
            "+-------------+-----------+------------+------------+------------+\n",
            "\n",
            "\n",
            "[KPI 2] Valor total por status (Delta)\n",
            "+---------+-----------+-----------+------------+\n",
            "|status   |qtd_pedidos|valor_total|ticket_medio|\n",
            "+---------+-----------+-----------+------------+\n",
            "|CANCELLED|1213       |617289.39  |508.89      |\n",
            "|PENDING  |1200       |613652.57  |511.38      |\n",
            "|SHIPPED  |1175       |605346.53  |515.19      |\n",
            "|DELIVERED|1149       |570071.61  |496.15      |\n",
            "+---------+-----------+-----------+------------+\n",
            "\n",
            "\n",
            "================ COMPARATIVO PARQUET x DELTA ================\n",
            "\n",
            "[PARQUET - RAW] Resumo financeiro\n",
            "+-------------+-----------+------------+\n",
            "|total_pedidos|valor_total|ticket_medio|\n",
            "+-------------+-----------+------------+\n",
            "|5000         |2456700.96 |506.85      |\n",
            "+-------------+-----------+------------+\n",
            "\n",
            "Tempo PARQUET: 0.0113 segundos\n",
            "\n",
            "[DELTA - CURATED] Resumo financeiro\n",
            "+-------------+-----------+------------+\n",
            "|total_pedidos|valor_total|ticket_medio|\n",
            "+-------------+-----------+------------+\n",
            "|4737         |2406360.10 |507.99      |\n",
            "+-------------+-----------+------------+\n",
            "\n",
            "Tempo DELTA:   0.0284 segundos\n",
            "\n",
            "Observação: aqui não buscamos benchmark rigoroso,\n",
            "mas evidenciar a diferença entre consultar a camada RAW em Parquet\n",
            "e a camada tratada em Delta Lake, que além do desempenho oferece ACID\n",
            "e melhor gerenciamento de metadados para o Lakehouse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 6 - OPERAÇÕES ACID (INSERT, DELETE, UPSERT) + TIME TRAVEL\n",
        "\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.functions import to_timestamp, col\n",
        "from datetime import datetime\n",
        "\n",
        "path_curated_delta = \"/tmp/vendas_curated_delta\"\n",
        "\n",
        "# Carregar a tabela Delta como DeltaTable (necessário para usar MERGE/DELETE)\n",
        "delta_table = DeltaTable.forPath(spark, path_curated_delta)\n",
        "\n",
        "print(\"Estado inicial (5 primeiros registros):\")\n",
        "delta_table.toDF().select(\"order_id\", \"customer_id\", \"amount\", \"order_date\", \"status\") \\\n",
        "    .show(5, truncate=False)\n",
        "\n",
        "# Guardar a versão INICIAL (primeira escrita da tabela) para usar no Time Travel.\n",
        "# Assim eu garanto que é uma versão realmente anterior às operações ACID.\n",
        "history_df = delta_table.history()\n",
        "versao_inicial = history_df.select(\"version\").orderBy(col(\"version\").asc()).limit(1).collect()[0][\"version\"]\n",
        "print(f\"\\nVersão inicial da tabela (antes das operações ACID desta célula): {versao_inicial}\")\n",
        "\n",
        "# 6.1 INSERT - inserir um novo pedido (ORD99999) se não existir\n",
        "print(\"\\n[ACID] INSERT - Novo pedido ORD99999\")\n",
        "\n",
        "df_insert = spark.createDataFrame(\n",
        "    [\n",
        "        (\"ORD99999\", \"CUST101\", 999.99, \"2024-12-01 10:00:00\", \"PENDING\")\n",
        "    ],\n",
        "    [\"order_id\", \"customer_id\", \"amount\", \"order_date\", \"status\"]\n",
        ")\n",
        "\n",
        "# Ajusto os tipos para ficar consistente com a camada CURATED\n",
        "df_insert = (\n",
        "    df_insert\n",
        "    .withColumn(\"amount\", col(\"amount\").cast(\"DECIMAL(10,2)\"))\n",
        "    .withColumn(\"order_date\", to_timestamp(\"order_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        ")\n",
        "\n",
        "# MERGE para tratar o INSERT de forma ACID: se não encontrar a chave, insere.\n",
        "delta_table.alias(\"t\").merge(\n",
        "    df_insert.alias(\"s\"),\n",
        "    \"t.order_id = s.order_id\"\n",
        ").whenNotMatchedInsertAll().execute()\n",
        "\n",
        "print(\"Registro ORD99999 após INSERT:\")\n",
        "delta_table.toDF().filter(col(\"order_id\") == \"ORD99999\").show(truncate=False)\n",
        "\n",
        "# 6.2 DELETE - remover um pedido específico (ORD00003)\n",
        "print(\"\\n[ACID] DELETE - Remover pedido ORD00003\")\n",
        "delta_table.delete(\"order_id = 'ORD00003'\")\n",
        "\n",
        "print(\"Verificando ORD00003 (deve estar vazio):\")\n",
        "delta_table.toDF().filter(col(\"order_id\") == \"ORD00003\").show(truncate=False)\n",
        "\n",
        "# 6.3 UPSERT - atualizar pedido existente ORD00001 (ou inserir se não existir)\n",
        "print(\"\\n[ACID] UPSERT - Atualizar pedido ORD00001\")\n",
        "\n",
        "df_upsert = spark.createDataFrame(\n",
        "    [\n",
        "        (\"ORD00001\", \"CUST001\", 1500.00, \"2024-11-24 15:30:00\", \"DELIVERED\")\n",
        "    ],\n",
        "    [\"order_id\", \"customer_id\", \"amount\", \"order_date\", \"status\"]\n",
        ")\n",
        "\n",
        "df_upsert = (\n",
        "    df_upsert\n",
        "    .withColumn(\"amount\", col(\"amount\").cast(\"DECIMAL(10,2)\"))\n",
        "    .withColumn(\"order_date\", to_timestamp(\"order_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
        ")\n",
        "\n",
        "# MERGE como UPSERT:\n",
        "# - quando encontra a chave, atualiza (UPDATE)\n",
        "# - se não encontrar, insere (INSERT).\n",
        "delta_table.alias(\"t\").merge(\n",
        "    df_upsert.alias(\"s\"),\n",
        "    \"t.order_id = s.order_id\"\n",
        ").whenMatchedUpdateAll() \\\n",
        " .whenNotMatchedInsertAll() \\\n",
        " .execute()\n",
        "\n",
        "print(\"Registro ORD00001 após UPSERT:\")\n",
        "delta_table.toDF().filter(col(\"order_id\") == \"ORD00001\").show(truncate=False)\n",
        "\n",
        "# 6.4 TIME TRAVEL - consultar a versão anterior da tabela (versão inicial)\n",
        "\n",
        "print(\"\\n================ TIME TRAVEL (VERSÃO INICIAL) ================\")\n",
        "\n",
        "df_historico = (\n",
        "    spark.read.format(\"delta\")\n",
        "    .option(\"versionAsOf\", versao_inicial)\n",
        "    .load(path_curated_delta)\n",
        ")\n",
        "\n",
        "print(f\"\\nNa versão {versao_inicial}, antes das operações ACID desta célula, o pedido ORD00003 existia?\")\n",
        "df_historico.filter(col(\"order_id\") == \"ORD00003\") \\\n",
        "    .select(\"order_id\", \"customer_id\", \"amount\", \"order_date\", \"status\") \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(f\"\\nNa versão {versao_inicial}, o pedido ORD99999 ainda não existia:\")\n",
        "df_historico.filter(col(\"order_id\") == \"ORD99999\") \\\n",
        "    .select(\"order_id\", \"customer_id\", \"amount\", \"order_date\", \"status\") \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "print(\"\\nVersão atual (após operações ACID) - registros afetados:\")\n",
        "delta_table.toDF().filter(\n",
        "    col(\"order_id\").isin(\"ORD00001\", \"ORD00003\", \"ORD99999\")\n",
        ").select(\"order_id\", \"customer_id\", \"amount\", \"order_date\", \"status\") \\\n",
        " .orderBy(\"order_id\").show(truncate=False)\n",
        "\n",
        "print(\"\\nTime Travel demonstra que o Delta Lake mantém o histórico de versões,\")\n",
        "print(\"permitindo recuperar o estado anterior da tabela mesmo após DELETE e UPSERT.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzgVsMeMlO1z",
        "outputId": "0bfccfe6-a9b9-4cdf-b389-7c6e4514ad85"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estado inicial (5 primeiros registros):\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "|order_id|customer_id|amount |order_date         |status   |\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "|ORD00001|CUST001    |1500.00|2024-11-24 15:30:00|DELIVERED|\n",
            "|ORD00004|CUST1      |163.86 |2024-11-11 00:45:46|SHIPPED  |\n",
            "|ORD00005|CUST98     |97.28  |2024-10-10 09:21:55|DELIVERED|\n",
            "|ORD00006|CUST6      |538.55 |2024-11-06 18:02:56|DELIVERED|\n",
            "|ORD00007|CUST80     |364.83 |2024-10-19 16:03:53|PENDING  |\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Versão inicial da tabela (antes das operações ACID desta célula): 0\n",
            "\n",
            "[ACID] INSERT - Novo pedido ORD99999\n",
            "Registro ORD99999 após INSERT:\n",
            "+--------+-----------+------+-------------------+-------+\n",
            "|order_id|customer_id|amount|order_date         |status |\n",
            "+--------+-----------+------+-------------------+-------+\n",
            "|ORD99999|CUST101    |999.99|2024-12-01 10:00:00|PENDING|\n",
            "+--------+-----------+------+-------------------+-------+\n",
            "\n",
            "\n",
            "[ACID] DELETE - Remover pedido ORD00003\n",
            "Verificando ORD00003 (deve estar vazio):\n",
            "+--------+-----------+------+----------+------+\n",
            "|order_id|customer_id|amount|order_date|status|\n",
            "+--------+-----------+------+----------+------+\n",
            "+--------+-----------+------+----------+------+\n",
            "\n",
            "\n",
            "[ACID] UPSERT - Atualizar pedido ORD00001\n",
            "Registro ORD00001 após UPSERT:\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "|order_id|customer_id|amount |order_date         |status   |\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "|ORD00001|CUST001    |1500.00|2024-11-24 15:30:00|DELIVERED|\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "\n",
            "\n",
            "================ TIME TRAVEL (VERSÃO INICIAL) ================\n",
            "\n",
            "Na versão 0, antes das operações ACID desta célula, o pedido ORD00003 existia?\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "|order_id|customer_id|amount|order_date         |status   |\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "|ORD00003|CUST92     |547.22|2024-10-22 09:39:49|DELIVERED|\n",
            "+--------+-----------+------+-------------------+---------+\n",
            "\n",
            "\n",
            "Na versão 0, o pedido ORD99999 ainda não existia:\n",
            "+--------+-----------+------+----------+------+\n",
            "|order_id|customer_id|amount|order_date|status|\n",
            "+--------+-----------+------+----------+------+\n",
            "+--------+-----------+------+----------+------+\n",
            "\n",
            "\n",
            "Versão atual (após operações ACID) - registros afetados:\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "|order_id|customer_id|amount |order_date         |status   |\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "|ORD00001|CUST001    |1500.00|2024-11-24 15:30:00|DELIVERED|\n",
            "|ORD99999|CUST101    |999.99 |2024-12-01 10:00:00|PENDING  |\n",
            "+--------+-----------+-------+-------------------+---------+\n",
            "\n",
            "\n",
            "Time Travel demonstra que o Delta Lake mantém o histórico de versões,\n",
            "permitindo recuperar o estado anterior da tabela mesmo após DELETE e UPSERT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 7 - RESUMO DO PIPELINE\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RESUMO DO PIPELINE - DATA LAKEHOUSE TRANSACIONAL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n1) Coleta / Aquisição (Camada RAW)\")\n",
        "print(\"   - Geração de 5000 pedidos simulados (order_id, customer_id, amount, order_date, status).\")\n",
        "print(\"   - Introdução de nulos propositalmente em customer_id, amount e order_date.\")\n",
        "print(\"   - Persistência em formato aberto Parquet na camada RAW: /tmp/vendas_raw.\")\n",
        "\n",
        "print(\"\\n2) Pré-processamento (Qualidade de Dados) - 3 Transformações\")\n",
        "print(\"   [1] Limpeza de nulos: remoção de registros com amount ou order_date nulos.\")\n",
        "print(\"   [2] Casting de tipos: amount -> DECIMAL(10,2), order_date -> TIMESTAMP.\")\n",
        "print(\"   [3] Desduplicação: uso de Window + row_number para manter o registro mais recente por order_id.\")\n",
        "\n",
        "print(\"\\n3) Camada CURATED em Delta Lake (Open Data Format)\")\n",
        "print(\"   - Dados tratados gravados em Delta Lake: /tmp/vendas_curated_delta.\")\n",
        "print(\"   - Delta Lake garante ACID, time travel e log de transações (_delta_log).\")\n",
        "\n",
        "print(\"\\n4) Análises de Negócio (KPIs)\")\n",
        "print(\"   - KPI 1: resumo financeiro geral (total de pedidos, valor total, ticket médio, menor e maior compra).\")\n",
        "print(\"   - KPI 2: valor total, quantidade de pedidos e ticket médio por status (PENDING, SHIPPED, DELIVERED, CANCELLED).\")\n",
        "\n",
        "print(\"\\n5) Operações ACID em Delta Lake (Requisito Avançado)\")\n",
        "print(\"   - INSERT: inclusão do pedido ORD99999 via MERGE quando não correspondido.\")\n",
        "print(\"   - DELETE: remoção lógica/física do pedido ORD00003 na tabela Delta.\")\n",
        "print(\"   - UPSERT: atualização do pedido ORD00001 (valor e status) via MERGE (whenMatchedUpdateAll).\")\n",
        "\n",
        "print(\"\\n6) Conclusão\")\n",
        "print(\"   - Pipeline completo de Data Lakehouse Transacional implementado em PySpark + Delta Lake.\")\n",
        "print(\"   - Atende aos requisitos: coleta, 3 transformações de qualidade, uso de formato Open Data (Delta),\")\n",
        "print(\"     e demonstração de operações ACID (INSERT, DELETE, UPSERT) na tabela final.\")\n",
        "\n",
        "print(\"\\n7) Alunos\")\n",
        "print(\"   - Arthur Peres            - 10310501\")\n",
        "print(\"   - João Vitor Camargo      - 10739829\")\n",
        "print(\"   - Ricardo Betteloni Lopes - 10742658\")\n",
        "print(\"=\" * 70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXb5tiQ0mKts",
        "outputId": "2f46423a-1917-453b-db18-67356f4e6b5a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "RESUMO DO PIPELINE - DATA LAKEHOUSE TRANSACIONAL\n",
            "======================================================================\n",
            "\n",
            "1) Coleta / Aquisição (Camada RAW)\n",
            "   - Geração de 5000 pedidos simulados (order_id, customer_id, amount, order_date, status).\n",
            "   - Introdução de nulos propositalmente em customer_id, amount e order_date.\n",
            "   - Persistência em formato aberto Parquet na camada RAW: /tmp/vendas_raw.\n",
            "\n",
            "2) Pré-processamento (Qualidade de Dados) - 3 Transformações\n",
            "   [1] Limpeza de nulos: remoção de registros com amount ou order_date nulos.\n",
            "   [2] Casting de tipos: amount -> DECIMAL(10,2), order_date -> TIMESTAMP.\n",
            "   [3] Desduplicação: uso de Window + row_number para manter o registro mais recente por order_id.\n",
            "\n",
            "3) Camada CURATED em Delta Lake (Open Data Format)\n",
            "   - Dados tratados gravados em Delta Lake: /tmp/vendas_curated_delta.\n",
            "   - Delta Lake garante ACID, time travel e log de transações (_delta_log).\n",
            "\n",
            "4) Análises de Negócio (KPIs)\n",
            "   - KPI 1: resumo financeiro geral (total de pedidos, valor total, ticket médio, menor e maior compra).\n",
            "   - KPI 2: valor total, quantidade de pedidos e ticket médio por status (PENDING, SHIPPED, DELIVERED, CANCELLED).\n",
            "\n",
            "5) Operações ACID em Delta Lake (Requisito Avançado)\n",
            "   - INSERT: inclusão do pedido ORD99999 via MERGE quando não correspondido.\n",
            "   - DELETE: remoção lógica/física do pedido ORD00003 na tabela Delta.\n",
            "   - UPSERT: atualização do pedido ORD00001 (valor e status) via MERGE (whenMatchedUpdateAll).\n",
            "\n",
            "6) Conclusão\n",
            "   - Pipeline completo de Data Lakehouse Transacional implementado em PySpark + Delta Lake.\n",
            "   - Atende aos requisitos: coleta, 3 transformações de qualidade, uso de formato Open Data (Delta),\n",
            "     e demonstração de operações ACID (INSERT, DELETE, UPSERT) na tabela final.\n",
            "\n",
            "7) Alunos\n",
            "   - Arthur Peres            - 10310501\n",
            "   - João Vitor Camargo      - 10739829\n",
            "   - Ricardo Betteloni Lopes - 10742658\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}